{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120150d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Directory containing the JSON files\n",
    "ekg_dir = 'ekg_files'\n",
    "\n",
    "# List to store all DataFrames\n",
    "dfs = []\n",
    "for filename in os.listdir(ekg_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(ekg_dir, filename), 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        # Take the first lead (usually \"I\")\n",
    "        if 'leads' in data and len(data['leads']) > 0:\n",
    "            lead = data['leads'][0]\n",
    "            signal = lead.get('signal', [])\n",
    "            # Remove None values from the signal\n",
    "            clean_signal = [x for x in signal if x is not None]\n",
    "            df_tmp = pd.DataFrame({'Signal': clean_signal})\n",
    "            dfs.append([filename.replace('_','/').replace('.json',''), df_tmp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b202aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest signal is from 70685/2021 with length 756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_length = float('inf')\n",
    "min_kg = None\n",
    "for kg, signal_df in dfs:\n",
    "    if len(signal_df) < min_length:\n",
    "        min_length = len(signal_df)\n",
    "        min_kg = kg\n",
    "print(f\"Shortest signal is from {min_kg} with length {min_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4065c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DANE_mpsi.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967f72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from Dane MPSI (df['zgon']) and join with signals from dfs using filename and KG\n",
    "\n",
    "# Prepare a mapping from KG to label\n",
    "kg_to_label = dict(zip(df['KG'], df['zgon']))\n",
    "\n",
    "# Prepare dataset: list of (signal, label) tuples\n",
    "signal_label_dataset = []\n",
    "for kg, signal_df in dfs:\n",
    "    if kg in kg_to_label:\n",
    "        label = kg_to_label[kg]\n",
    "        signal = signal_df['Signal'].values\n",
    "        signal_label_dataset.append((signal, label))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EKGDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # list of (signal, label) tuples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal, label = self.data[idx]\n",
    "        # Convert signal to float32 tensor and add channel dimension\n",
    "        signal_tensor = torch.tensor(signal, dtype=torch.float32).unsqueeze(-1)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return signal_tensor, label_tensor\n",
    "signal_label_dataset = []\n",
    "for kg, signal_df in dfs:\n",
    "    if kg in kg_to_label:\n",
    "        label = kg_to_label[kg]\n",
    "        signal = signal_df['Signal'].values[-756:]  # take last 756 elements\n",
    "        signal_label_dataset.append((signal, label))\n",
    "\n",
    "ekg_dataset = EKGDataset(signal_label_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2809706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6939\n",
      "Epoch 2/100, Loss: 0.6926\n",
      "Epoch 3/100, Loss: 0.6927\n",
      "Epoch 4/100, Loss: 0.6927\n",
      "Epoch 5/100, Loss: 0.6924\n",
      "Epoch 6/100, Loss: 0.6925\n",
      "Epoch 7/100, Loss: 0.6924\n",
      "Epoch 8/100, Loss: 0.6926\n",
      "Epoch 9/100, Loss: 0.6921\n",
      "Epoch 10/100, Loss: 0.6926\n",
      "Validation Loss: 0.6940, Accuracy: 0.5190\n",
      "Epoch 11/100, Loss: 0.6922\n",
      "Epoch 12/100, Loss: 0.6921\n",
      "Epoch 13/100, Loss: 0.6924\n",
      "Epoch 14/100, Loss: 0.6922\n",
      "Epoch 15/100, Loss: 0.6918\n",
      "Epoch 16/100, Loss: 0.6920\n",
      "Epoch 17/100, Loss: 0.6919\n",
      "Epoch 18/100, Loss: 0.6918\n",
      "Epoch 19/100, Loss: 0.6918\n",
      "Epoch 20/100, Loss: 0.6921\n",
      "Validation Loss: 0.6947, Accuracy: 0.5316\n",
      "Epoch 21/100, Loss: 0.6917\n",
      "Epoch 22/100, Loss: 0.6920\n",
      "Epoch 23/100, Loss: 0.6923\n",
      "Epoch 24/100, Loss: 0.6916\n",
      "Epoch 25/100, Loss: 0.6915\n",
      "Epoch 26/100, Loss: 0.6919\n",
      "Epoch 27/100, Loss: 0.6920\n",
      "Epoch 28/100, Loss: 0.6917\n",
      "Epoch 29/100, Loss: 0.6915\n",
      "Epoch 30/100, Loss: 0.6919\n",
      "Validation Loss: 0.6953, Accuracy: 0.5316\n",
      "Epoch 31/100, Loss: 0.6916\n",
      "Epoch 32/100, Loss: 0.6914\n",
      "Epoch 33/100, Loss: 0.6915\n",
      "Epoch 34/100, Loss: 0.6915\n",
      "Epoch 35/100, Loss: 0.6910\n",
      "Epoch 36/100, Loss: 0.6912\n",
      "Epoch 37/100, Loss: 0.6917\n",
      "Epoch 38/100, Loss: 0.6913\n",
      "Epoch 39/100, Loss: 0.6913\n",
      "Epoch 40/100, Loss: 0.6910\n",
      "Validation Loss: 0.6960, Accuracy: 0.5316\n",
      "Epoch 41/100, Loss: 0.6915\n",
      "Epoch 42/100, Loss: 0.6911\n",
      "Epoch 43/100, Loss: 0.6909\n",
      "Epoch 44/100, Loss: 0.6913\n",
      "Epoch 45/100, Loss: 0.6912\n",
      "Epoch 46/100, Loss: 0.6911\n",
      "Epoch 47/100, Loss: 0.6906\n",
      "Epoch 48/100, Loss: 0.6908\n",
      "Epoch 49/100, Loss: 0.6909\n",
      "Epoch 50/100, Loss: 0.6922\n",
      "Validation Loss: 0.6967, Accuracy: 0.5316\n",
      "Epoch 51/100, Loss: 0.6920\n",
      "Epoch 52/100, Loss: 0.6907\n",
      "Epoch 53/100, Loss: 0.6903\n",
      "Epoch 54/100, Loss: 0.6912\n",
      "Epoch 55/100, Loss: 0.6895\n",
      "Epoch 56/100, Loss: 0.6916\n",
      "Epoch 57/100, Loss: 0.6908\n",
      "Epoch 58/100, Loss: 0.6908\n",
      "Epoch 59/100, Loss: 0.6908\n",
      "Epoch 60/100, Loss: 0.6913\n",
      "Validation Loss: 0.6975, Accuracy: 0.5316\n",
      "Epoch 61/100, Loss: 0.6905\n",
      "Epoch 62/100, Loss: 0.6903\n",
      "Epoch 63/100, Loss: 0.6910\n",
      "Epoch 64/100, Loss: 0.6906\n",
      "Epoch 65/100, Loss: 0.6918\n",
      "Epoch 66/100, Loss: 0.6899\n",
      "Epoch 67/100, Loss: 0.6896\n",
      "Epoch 68/100, Loss: 0.6897\n",
      "Epoch 69/100, Loss: 0.6877\n",
      "Epoch 70/100, Loss: 0.6983\n",
      "Validation Loss: 0.7301, Accuracy: 0.5190\n",
      "Epoch 71/100, Loss: 0.7337\n",
      "Epoch 72/100, Loss: 0.7142\n",
      "Epoch 73/100, Loss: 0.6992\n",
      "Epoch 74/100, Loss: 0.6995\n",
      "Epoch 75/100, Loss: 0.6944\n",
      "Epoch 76/100, Loss: 0.6939\n",
      "Epoch 77/100, Loss: 0.6950\n",
      "Epoch 78/100, Loss: 0.6922\n",
      "Epoch 79/100, Loss: 0.6934\n",
      "Epoch 80/100, Loss: 0.6932\n",
      "Validation Loss: 0.6928, Accuracy: 0.5190\n",
      "Epoch 81/100, Loss: 0.6930\n",
      "Epoch 82/100, Loss: 0.6896\n",
      "Epoch 83/100, Loss: 0.6897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     43\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Filip\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:570\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward\u001b[39m(\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    572\u001b[0m ):\n\u001b[0;32m    573\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple RNN model if not already defined\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # Take the last output\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SimpleRNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = epochs if 'epochs' in locals() else 10\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(ekg_dataset))\n",
    "val_size = len(ekg_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(ekg_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for signals, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signals)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in val_loader:\n",
    "                outputs = model(signals)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6423e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6964\n",
      "Epoch 2/30, Loss: 0.6932\n",
      "Epoch 3/30, Loss: 0.6922\n",
      "Epoch 4/30, Loss: 0.6928\n",
      "Epoch 5/30, Loss: 0.6931\n",
      "Validation Loss: 0.6932, Accuracy: 0.5190\n",
      "Epoch 6/30, Loss: 0.6925\n",
      "Epoch 7/30, Loss: 0.6923\n",
      "Epoch 8/30, Loss: 0.6927\n",
      "Epoch 9/30, Loss: 0.6923\n",
      "Epoch 10/30, Loss: 0.6921\n",
      "Validation Loss: 0.6937, Accuracy: 0.5190\n",
      "Epoch 11/30, Loss: 0.6923\n",
      "Epoch 12/30, Loss: 0.6933\n",
      "Epoch 13/30, Loss: 0.6921\n",
      "Epoch 14/30, Loss: 0.6916\n",
      "Epoch 15/30, Loss: 0.6916\n",
      "Validation Loss: 0.6940, Accuracy: 0.5316\n",
      "Epoch 16/30, Loss: 0.6915\n",
      "Epoch 17/30, Loss: 0.6915\n",
      "Epoch 18/30, Loss: 0.6908\n",
      "Epoch 19/30, Loss: 0.6899\n",
      "Epoch 20/30, Loss: 0.6899\n",
      "Validation Loss: 0.6938, Accuracy: 0.4937\n",
      "Epoch 21/30, Loss: 0.6893\n",
      "Epoch 22/30, Loss: 0.6889\n",
      "Epoch 23/30, Loss: 0.6877\n",
      "Epoch 24/30, Loss: 0.6865\n",
      "Epoch 25/30, Loss: 0.6850\n",
      "Validation Loss: 0.6942, Accuracy: 0.5316\n",
      "Epoch 26/30, Loss: 0.6853\n",
      "Epoch 27/30, Loss: 0.6851\n",
      "Epoch 28/30, Loss: 0.6837\n",
      "Epoch 29/30, Loss: 0.6837\n",
      "Epoch 30/30, Loss: 0.6820\n",
      "Validation Loss: 0.6938, Accuracy: 0.5316\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple GRU model\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, output_size=1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]  # Take the last output\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "gru_model = SimpleGRU()\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    gru_model.train()\n",
    "    total_loss = 0\n",
    "    for signals, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = gru_model(signals)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        gru_model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in val_loader:\n",
    "                outputs = gru_model(signals)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
